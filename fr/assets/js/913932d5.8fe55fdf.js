"use strict";(self.webpackChunkpromptgineering=self.webpackChunkpromptgineering||[]).push([[2887],{27646:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>m});var i=a(87462),r=(a(67294),a(3905)),n=a(39145);const o={sidebar_position:1},s="\ud83d\udfe2 Introduction",l={unversionedId:"reliability/intro",id:"reliability/intro",title:"\ud83d\udfe2 Introduction",description:"Ce chapitre couvre comment rendre les compl\xe9tions plus fiables, ainsi que la mani\xe8re d'impl\xe9menter des contr\xf4les pour assurer la fiabilit\xe9 des r\xe9sultats.",source:"@site/i18n/fr/docusaurus-plugin-content-docs/current/reliability/intro.md",sourceDirName:"reliability",slug:"/reliability/intro",permalink:"/fr/docs/reliability/intro",draft:!1,editUrl:"https://github.com/trigaten/promptgineering/tree/v1.2.3/docs/reliability/intro.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"\u2696\ufe0f Reliability",permalink:"/fr/docs/category/\ufe0f-reliability"},next:{title:"\ud83d\udfe2 Prompt Debiasing",permalink:"/fr/docs/reliability/debiasing"}},p={},m=[],u={toc:m},c="wrapper";function d(e){let{components:t,...a}=e;return(0,r.kt)(c,(0,i.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"-introduction"},"\ud83d\udfe2 Introduction"),(0,r.kt)("p",null,"Ce chapitre couvre comment rendre les compl\xe9tions plus fiables, ainsi que la mani\xe8re d'impl\xe9menter des contr\xf4les pour assurer la fiabilit\xe9 des r\xe9sultats."),(0,r.kt)("p",null,"Dans une certaine mesure, la plupart des techniques pr\xe9c\xe9demment abord\xe9es ont pour but d'am\xe9liorer la pr\xe9cision des compl\xe9tions, et donc leur fiabilit\xe9, en particulier l'auto-consistance",(0,r.kt)("sup",{parentName:"p",id:"fnref-1"},(0,r.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1")),". Cependant, il existe un certain nombre d'autres techniques qui peuvent \xeatre utilis\xe9es pour am\xe9liorer la fiabilit\xe9, au-del\xe0 des strat\xe9gies de prompting de base."),(0,r.kt)("p",null,"Les ",(0,r.kt)("a",{parentName:"p",id:"LLM_0_4_1708300444158","data-tooltip-html":"Large Language Model. A model that is trained to predict the next word in a sentence.","data-tooltip-place":"top"},"LLMs"),(0,r.kt)(n.u,{anchorId:"LLM_0_4_1708300444158",clickable:!0,mdxType:"Tooltip"})," se sont r\xe9v\xe9l\xe9s \xeatre plus fiables que ce que nous pourrions attendre en interpr\xe9tant ce qu'un prompt ",(0,r.kt)("em",{parentName:"p"},"essaie")," de dire lorsqu'ils r\xe9pondent \xe0 des prompts mal orthographi\xe9s, mal formul\xe9s ou m\xeame activement trompeurs",(0,r.kt)("sup",{parentName:"p",id:"fnref-2"},(0,r.kt)("a",{parentName:"sup",href:"#fn-2",className:"footnote-ref"},"2")),". Malgr\xe9 cette capacit\xe9, ils pr\xe9sentent encore divers probl\xe8mes, y compris des hallucinations",(0,r.kt)("sup",{parentName:"p",id:"fnref-3"},(0,r.kt)("a",{parentName:"sup",href:"#fn-3",className:"footnote-ref"},"3")),", des explications erron\xe9es avec les m\xe9thodes de ",(0,r.kt)("a",{parentName:"p",id:"CoT prompting_9_49_1708300444158","data-tooltip-html":"The main idea of CoT is that by showing the LLM some few shot exemplars where the reasoning process is explained in the exemplars, the LLM will also show the reasoning process when answering the prompt.","data-tooltip-place":"top"},"CoT"),(0,r.kt)(n.u,{anchorId:"CoT prompting_9_49_1708300444158",clickable:!0,mdxType:"Tooltip"}),(0,r.kt)("sup",{parentName:"p",id:"fnref-3"},(0,r.kt)("a",{parentName:"sup",href:"#fn-3",className:"footnote-ref"},"3")),", et de multiples biais, y compris le biais de l'\xe9tiquette majoritaire, le biais de r\xe9cence et le biais de jeton commun",(0,r.kt)("sup",{parentName:"p",id:"fnref-4"},(0,r.kt)("a",{parentName:"sup",href:"#fn-4",className:"footnote-ref"},"4")),". De plus, le CoT en zero-shot peut \xeatre particuli\xe8rement biais\xe9 lorsqu'il traite de sujets sensibles",(0,r.kt)("sup",{parentName:"p",id:"fnref-5"},(0,r.kt)("a",{parentName:"sup",href:"#fn-5",className:"footnote-ref"},"5")),"."),(0,r.kt)("p",null,"Parmi les solutions courantes \xe0 certains de ces probl\xe8mes figurent les calibrateurs pour \xe9liminer les biais ",(0,r.kt)("em",{parentName:"p"},"a priori"),", et les v\xe9rificateurs pour \xe9valuer les compl\xe9tions, ainsi que la promotion de la diversit\xe9 dans les compl\xe9tions."),(0,r.kt)("div",{className:"footnotes"},(0,r.kt)("hr",{parentName:"div"}),(0,r.kt)("ol",{parentName:"div"},(0,r.kt)("li",{parentName:"ol",id:"fn-1"},"Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., & Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-1",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-2"},"Webson, A., Loo, A. M., Yu, Q., & Pavlick, E. (2023). Are Language Models Worse than Humans at Following Prompts? It\u2019s Complicated. arXiv:2301.07085v1 [Cs.CL].\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-2",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-3"},"Ye, X., & Durrett, G. (2022). The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-3",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-4"},"Zhao, T. Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021). Calibrate Before Use: Improving Few-Shot Performance of Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-4",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-5"},"Shaikh, O., Zhang, H., Held, W., Bernstein, M., & Yang, D. (2022). On Second Thought, Let\u2019s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-5",className:"footnote-backref"},"\u21a9")))))}d.isMDXComponent=!0}}]);